{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function gradient_descent:\n",
    "**Arguments** : <br>\n",
    "gradient : First Derivative of optimazation function<br>\n",
    "init_ : Initial value of parameter<br>\n",
    "learn_rate : Learning rate<br>\n",
    "n_iter  : Number of iteration <br>\n",
    "tol : Minimum absolut value of delta<br><br>\n",
    "**Return** : <br>\n",
    "x : Value of parameter for which gradient is zero(Rounding upto 3 decimal places)<br><br>\n",
    "**Functinality**:<br>\n",
    "This function is used to find the value of x for which the gradient is zero or less than absolute value delta.<br>\n",
    "If function doesn't converges in n_iter then retured value may not be optional<br><br>\n",
    "**Algorithm** :<br>\n",
    "1. Initialze the value of x\n",
    "2. for each iteration\n",
    "    1. Calculate the delta as the amount of weight updated\n",
    "    2. If absolute value of delta is less than tol\n",
    "        1. Break\n",
    "    4. Move in the negative direction of gradient i.e., delta\n",
    "7. Return the final value of x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, init_, learn_rate, n_iter = 50, tol = 1e-06):\n",
    "    x = init_                                                                  #Initializing the parameter\n",
    "    for _ in range(n_iter):\n",
    "        delta = - learn_rate * gradient(x)                       #Calculating the update using grdient\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break                                                           #If value ofupdate is less than tol then break\n",
    "        x += delta                                                        #Updating the parameter\n",
    "    return round(x*1000) / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.(a).(i)\n",
    "Finding the value of x for which function f(x) is minimum.\n",
    "\\begin{equation}\n",
    "  f(x) = x^{2} + 3x + 4\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  f'(x) = 2x + 3\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(gradient=lambda v: 2 * v + 3, init_= 4.0, learn_rate=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Solution 1.(a).(ii)\n",
    "Finding the value of x for which function f(x) is minimum.\n",
    "\\begin{equation}\n",
    "  f(x) = x^{4} - 3x^{2} + 2x\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  f'(x) = 4x^3 - 6x + 2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.366"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(gradient=lambda v: ((4 * (v**3)) - (6 * v) + 2), init_= 4.0, learn_rate=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function linearRegression:\n",
    "**Arguments** : <br>\n",
    "X : Feature Matrix<br>\n",
    "Y : Label Vector<br>\n",
    "init_m : initial value of slope (defaulf : 0)<br>\n",
    "init_c : initial value of intercept (default : 0)<br>\n",
    "learning_rate : Learning Rate<br>\n",
    "n_iter : Number of iteration<br>\n",
    "tol : Minimum absolut value of update<br><br>\n",
    "**Return**:<br>\n",
    "m : Optimal value of slope <br>\n",
    "c : Optimal value of intercept<br><br>\n",
    "\n",
    "**Functionality**:<br>\n",
    "Finding the optimal line which fit the feature X and give the minimum eculdiean loss over label Y.<br>\n",
    "\n",
    "**Algorithm**:<br>\n",
    "1. Initialze m, c\n",
    "2. for each itertion in n_iter\n",
    "    1. Find the predicted label of Y using current m and c\n",
    "    2. Calculate the gradient of m <br> \\begin{equation} grad\\_m = -\\frac{2}{n}\\sum_{i=1}^{n}x (y - \\hat{y})  \\end{equation}<br>\n",
    "    3. calculate the gradient of c \\begin{equation} grad\\_c = -\\frac{2}{n}\\sum_{i=1}^{n} (y - \\hat{y})  \\end{equation} <br><br>\n",
    "    5. If Update in m and c both are less than tol then we are stop further updating and assumes the gradient is converged.<br><br>\n",
    "    6. Move in the negative direction of gradient for m  :  $m = m -  learning\\_rate * grad\\_m$\n",
    "    7. Move in the negative direction of gradient for c    :  $c = c -  learning\\_rate * grad\\_c$ <br><br>\n",
    "3. Return the optimal value of slope and intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegression(X, Y, init_m = 0, init_c = 0, learning_rate = 0.02, n_iter = 1000, tol = 1e-06):\n",
    "    \n",
    "    m = init_m                                                                                  #Initializing the parameter m (Slope)\n",
    "    c = init_c                                                                                    #Initializing the parameter c (intercept)\n",
    "    n = len(X)                                                                                  #Finding the length of X\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        Y_Pred = m * X + c                                                               #Predicting the value of y\n",
    "        \n",
    "        grad_m = -(2 / n) * np.sum(X * (Y - Y_Pred))                       #Calculating the gradient of m\n",
    "        \n",
    "        grad_c = -(2 / n) * np.sum(1 * (Y-Y_Pred))                           #Calculating the gradient of c\n",
    "        \n",
    "        if (abs(learning_rate * grad_m) < tol) and (abs(learning_rate * grad_c) < tol):\n",
    "            break                                                                                                                            #If update of m and c are less than tol --> break\n",
    "        \n",
    "        m = m - learning_rate * grad_m                                          #Updating the value of m\n",
    "        c = c - learning_rate * grad_c                                              #Updating the value of c\n",
    "        \n",
    "    return m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating artificial dataset for linear regrestion<br>\n",
    "X = Array of 10000 values with mean = 1.5 and standard deviatin = 2.5\n",
    "res = 10000 reidual terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5\n",
    "res = 1.5 * np.random.randn(10000)\n",
    "y = 2 + 0.3 * X + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = linearRegression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value of slope : a = 0.3\n",
      "Optimal Value of intercept : b = 2.02\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Value of slope : a = {}\\nOptimal Value of intercept : b = {}\".format(round(a, 2), round(b, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function batchLinearRegression:\n",
    "**Arguments** : <br>\n",
    "X : Feature Matrix<br>\n",
    "Y : Label Vector<br>\n",
    "batch_size : Size of batch that is being used to find the gradient<br>\n",
    "init_m : initial value of slope (defaulf : 0)<br>\n",
    "init_c : initial value of intercept (default : 0)<br>\n",
    "learning_rate : Learning Rate<br>\n",
    "n_iter : Number of iteration<br>\n",
    "tol : Minimum absolut value of update<br><br>\n",
    "**Return**:<br>\n",
    "m : Optimal value of slope <br>\n",
    "c : Optimal value of intercept<br><br>\n",
    "\n",
    "**Functionality**:<br>\n",
    "Finding the optimal line which fit the feature X and give the minimum eculdiean loss over label Y.<br>\n",
    "\n",
    "**Algorithm**:<br>\n",
    "1. Initialze m, c\n",
    "2. for each itertion in n_iter\n",
    "    1. Find the batch of feature (batch_X) and label (batch_Y)without replacement \n",
    "    2. Find the predicted label of batch_Y (batch_Y_Pred) using current m and c\n",
    "    3. Calculate the gradient of m <br> \\begin{equation} grad\\_m = -\\frac{2}{batch\\_size}\\sum_{i=1}^{batch\\_size}x (y - \\hat{y})  \\end{equation}<br>\n",
    "    4. calculate the gradient of c<br> \\begin{equation} grad\\_c = -\\frac{2}{batch\\_size}\\sum_{i=1}^{batch\\_size} (y - \\hat{y})  \\end{equation}<br><br>\n",
    "    5. If Update in m and c both are less than tol then we are stop further updating and assumes the gradient is converged.<br><br>\n",
    "    6. Move in the negative direction of gradient for m  :  $m = m -  learning\\_rate * grad\\_m$\n",
    "    7. Move in the negative direction of gradient for c    :  $c = c -  learning\\_rate * grad\\_c$<br><br>\n",
    "3. Return the optimal value of slope and intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchLinearRegression(X, Y, batch_size, init_m = 0, init_c = 0, learning_rate = 0.02, n_iter = 1000, tol = 1e-06):\n",
    "    \n",
    "    m = init_m                                                                                                  #Initializing the parameter m (Slope)\n",
    "    c = init_c                                                                                                     #Initializing the parameter c (intercept)\n",
    "    n = len(X)                                                                                                    #Finding the length of X\n",
    "    index = list(range(n))                                                                                  #Creating the list of index\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        rand_idx = np.random.choice(index, batch_size, replace=False)         #Randomly selecting the index for batch of batch_size\n",
    "        \n",
    "        batch_X = np.array([X[i] for i in rand_idx])                                           #Creating the batch_X using random_index\n",
    "        batch_Y = np.array([Y[i] for i in rand_idx])                                           #Creating the batch_Y using random_index\n",
    "\n",
    "        Y_Pred = m * batch_X + c                                                                    #Predicting the value of y\n",
    "        \n",
    "        grad_m = -(2 / batch_size) * np.sum(batch_X * (batch_Y - Y_Pred))    #Calculating the gradient of m\n",
    "        \n",
    "        grad_c = -(2 / batch_size) * np.sum(1 * (batch_Y - Y_Pred))                 #Calculating the gradient of c\n",
    "        \n",
    "        if (abs(learning_rate * grad_m) < tol) and (abs(learning_rate * grad_c) < tol):\n",
    "            break                                                                                                                  #If update of m and c are less than tol --> break\n",
    "        \n",
    "        m = m - learning_rate * grad_m                                                            #Updating the value of m\n",
    "        c = c - learning_rate * grad_c                                                               #Updating the value of c\n",
    "        \n",
    "    return m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value of slope : a = 0.36\n",
      "Optimal Value of intercept : b = 2.04\n"
     ]
    }
   ],
   "source": [
    "a, b = batchLinearRegression(X, y, batch_size=1, learning_rate=0.002, n_iter=2000)\n",
    "print(\"Optimal Value of slope : a = {}\\nOptimal Value of intercept : b = {}\".format(round(a, 2), round(b, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the time taken for the execution of Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for execution of SGD : 1.3235 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "a, b = batchLinearRegression(X, y, 1)\n",
    "toc = time.time()\n",
    "print(\"Total time for execution of SGD : {} seconds\".format(round(toc - tic, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the time taken for the execution of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for execution of GD : 2.997 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "a, b = batchLinearRegression(X, y, len(X))\n",
    "toc = time.time()\n",
    "print(\"Total time for execution of GD : {} seconds\".format(round(toc - tic, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two cell it can be clrealy seen that .**Stochastic Gradient Descent is faster than Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal batch size\n",
    "\n",
    "Parameter for finding the optimal batch size\n",
    "1. Having minimum mean squared error \n",
    "2. Execution in less time\n",
    "\n",
    "\n",
    "There is trade-off between accuracy and time for finding the optimal batch size.\n",
    "1. On increasing the batch size accuracy increases (because convergence is smoother with large batch).\n",
    "2. On increasing the batch size time complexity also increases (takes longer time to calculate the gradient of large batch)\n",
    "\n",
    "This means if SGD is faster but less accurate and GD is slower but vary accurate. Therefore we need to find out an optimal batch size which take less time GD and give good accuracy.\n",
    "\n",
    "**Note**<br>\n",
    "1.  We are defining the batch size in power of 2 (i.e., 1, 2, 8.....) because it is most efficient size in computer science.\n",
    "2. Record the time taken by each batch for calculating the gradient.\n",
    "3. Record the accuracy for each batch size\n",
    "\n",
    "Formula for calculating the Mean Squared error\n",
    "\\begin{equation} MSE = \\frac{1}{n}\\sum_{i = 1}^{n}(y - \\hat{y})^2 \\end{equation}\n",
    "\n",
    "We are executing the mini batch gradient descent **10** for each batch_size and taking average of for more accurate measurement of error and time taken by each batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = []                                                                   #List for recording the error for each batch\n",
    "avg_time_diff = []                                                                      #List for recording the error for each batch\n",
    "\n",
    "for i in range(14):                                                              #Looping over all batch size in power of 2 (less then len(X))\n",
    "   \n",
    "    mse= []                                                                     #For storing the mse for single batch_size\n",
    "    td = []                                                                        #For storing the time taken for single batch_size\n",
    "   \n",
    "    for k in range(10):                                                             #Looping 10 time over single batch size for more accurate measurement \n",
    "        \n",
    "        tic = time.time()                                                               #Starting the Time\n",
    "        m, c = batchLinearRegression(X, y, np.power(2, i))       #Finding the optimal slope and intercept for batch of batch_size = 2^i\n",
    "        toc = time.time()                                                              #Stoping the time\n",
    "    \n",
    "        ypred = (m * X) + c                                                        #Calculating the y_pred\n",
    "    \n",
    "        mse.append(np.sum((y - ypred) ** 2)/ len(y))               #Calculating the mean square error\n",
    "    \n",
    "        \n",
    "    \n",
    "        td.append(round((toc - tic) * 1000) / 1000)                    #Record the time taken by this batch\n",
    "        \n",
    "    avg_mse.append(np.mean(mse))                                    #appending the mean value of mse error \n",
    "    avg_time_diff.append(np.mean(td))                                 #appending the mean value of time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the rmse_error and time_diff using min-max scaler so that both come in same range and we can then plot it to find the optimal batch size<br>\n",
    "**Min-Max Scalar**\n",
    "\\begin{equation} x[i] = \\frac{x[i] - X_{min}}{X_{max} - X_{min}} \\end{equation}\n",
    "\n",
    "This will bring all the values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.array(avg_mse)\n",
    "rmse = (rmse - min(rmse)) / (max(rmse) - min(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetaken = np.array(avg_time_diff)\n",
    "timetaken = (timetaken - min(timetaken)) / (max(timetaken) - min(timetaken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the graph of normalized rmse _error and time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(14)), rmse)\n",
    "plt.plot(list(range(14)), timetaken)\n",
    "plt.xlabel(\"Log2(Batch Size)\")\n",
    "plt.ylabel(\"Normalized value of Error and Time taken\")\n",
    "plt.legend([\"rmse\", \"Time\"])\n",
    "plt.title(\"Trade-Off between Accuracy and Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion from the above graph\n",
    "1. Batch_size = 8192  :  give best accuracy but take longer time\n",
    "2. Batch_size = 1 or 2 or 4   :   give less accuracy but are fast\n",
    "\n",
    "From the graph we can say that **batch of size 32** will be optimal.\n",
    "\n",
    "#### But optimal batch size is an hyper parameter it varies according the problem defination other paramerter\n",
    "means it may be 32 this time but changes next time. After executing multiple time optimal batch size come from this set **{32, 64, 128}**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use short abbreviation for each event in the Bayesian Network for ease of readability \n",
    "1. S : Smokes\n",
    "2. LD : Lung Disease\n",
    "3. C : Cold\n",
    "4. Co :Cough\n",
    "5. F : Fever\n",
    "6. SB : Shortness of breath\n",
    "7. CP : Chest Pain\n",
    "\n",
    "### Solution 2.(i). The probability that someone has both cold and a fever\n",
    "\n",
    "In the Bayesian Network Fever is dependent on Cold. So, the probability that someone has both fever and cold can be calculated as \n",
    "\n",
    "\\begin{equation}P(C,F) = P(F |C)\\;P(C) \\end{equation}\n",
    "\n",
    "\\begin{equation} = 0.307 * 0.02 \\end{equation}\n",
    "\n",
    "\\begin{equation}= 0.00614 \\end{equation}\n",
    "\n",
    "### Solution 2.(ii). The probability that someone who has a cough has a cold\n",
    "\n",
    "\n",
    "The probability of having cold given that he(she) has Cough can be calculated as follows: \n",
    "\n",
    "\\begin{equation}P(C\\;|\\;Co) = \\frac{P(C, Co)}{P(Co)} ..........(1)\\end{equation}\n",
    "\n",
    "To calculated above probability we need to find $P(Co, C)$ and $P(Co)$. <br><br>\n",
    "\n",
    "\n",
    "From the Bayesian Network there is dependency of Lung Disease on Cough. So we need to first find out the $P(LD)$ and $P(\\overline{LD})$\n",
    "\n",
    "Since Lung Disease is dependent over Smokes, probability of having Lung Disease wil be:\n",
    "\n",
    "\\begin{equation} P(LD) = P(LD|S)P(S) + P(LD|\\overline {S})P(\\overline{S}) \\end{equation}\n",
    "\n",
    "\\begin{equation}P(LD) = 0.1009*0.2+0.001*0.8 = 0.02098 \\end{equation}\n",
    "\n",
    "and probability of not having Lung Disease will be $P(\\overline{LD}) = 1 - P(LD) = 0.97902$<br><br>\n",
    "\n",
    "\n",
    "\n",
    "Now, lets calculated probability that someone has both Cold and Cough i.e.,$P(Co,C)$\n",
    "\n",
    "\\begin{equation}  P(Co, C) = P(Co, C, LD) + P(Co, C, \\overline{LD} ) \\end{equation}\n",
    "\n",
    "From the Bayesian Network Cold and Lung Disease are not dependent then, \n",
    "\n",
    "\\begin{equation} P(Co, C) = P(Co|C, LD)P(C)P(LD) + P(Co|C, \\overline{LD})P(C)P(\\overline{LD}) \\end{equation}\n",
    "\n",
    "\\begin{equation}P(Co, C) = 0.7525*0.02*0.02098 + 0.505*0.02*0.97902\\end{equation}\n",
    "\n",
    "\\begin{equation} P(Co, C) = 0.010203851 ..........(2)\\end{equation}<br><br>\n",
    "\n",
    "\n",
    "\n",
    "Now calculate the probability of Denominator i.e., $P(Co)$\n",
    "\n",
    "\\begin{equation}P(Co) = P(Co, LD, C) + P(Co, LD, \\overline{C}) + P(Co, \\overline{LD}, C) + P(Co, \\overline{LD}, \\overline{C}) \\end{equation}\n",
    "\n",
    "\\begin{equation}P(Co) =  P(Co|LD, C)\\,P(LD)\\,P(C) + P(Co|LD, \\overline{C})\\,P(LD)\\,P(\\overline{C}) + P(Co|\\overline{LD},C)\\,P( \\overline{LD})\\,P(C) + P(Co|\\overline{LD},\\overline{C})\\,P(\\overline{LD})\\,P(\\overline{C})\\end{equation}\n",
    "\n",
    "\\begin{equation}P(Co) = 0.7525*0.02098*0.02 +  0.505*0.02098*0.98 +  0.505*0.97902*0.02 + 0.01*0.97902*0.98\\end{equation}\n",
    "\n",
    "\\begin{equation}P(Co) =0.000315749 + 0.010383002 +  0.009888102 + 0.009594396\\end{equation}\n",
    "\n",
    "\\begin{equation} P(Co) = 0.030181249 ..........(3)\\end{equation}<br><br>\n",
    "\n",
    "Now, putting values from equation (2) and (3) in equation (1) we get,  \n",
    "\n",
    "\\begin{equation}P(C\\;|\\;Co) = \\frac{P(C\\;and\\;Co)}{P(Co)} \\end{equation}\n",
    "\n",
    "\\begin{equation} = \\frac{0.010203851}{0.030181249}\\end{equation}\n",
    "\n",
    "\\begin{equation} = 0.338085776\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3.\n",
    "Let $X$ be a RV following multinomial distribution,<br> \n",
    "$K$ be the total  number of possible outcomes<br>\n",
    "$x_i$ be number of success of the $i^{th}$ outcomes in $n$ random trials<br>\n",
    "$p_i$ be the probability of success of the $i^{th}$ outcomes. Then, \n",
    "\n",
    "\\begin{equation} P(X = x|n, p) =  {n \\choose {x_1, x_2 ... x_K}}\\prod_{i=1}^Kp_i^{x_i} = n!\\prod_{i = 1}^K\\frac{p_i^{x_i}}{x_i!}\\end{equation}\n",
    "\n",
    "where, \n",
    "\n",
    "\\begin{equation} \\sum_{i=1}^Kx_i = n \\end{equation}and \\begin{equation}  \\sum_{i = 1}^Kp_i = 1\\end{equation}\n",
    "\n",
    "The log likelihood of multinomial distribution is given as, \n",
    "\n",
    "\\begin{equation} \\mathcal{LL}(p) = \\log\\left({{n \\choose {x_1, x_2 ... x_k}}\\prod_{i = 1}^Kp_i^{x_i}}\\right) \\end{equation}\n",
    "\n",
    "\\begin{equation}  = \\log{n \\choose{x_1, x_2...x_K}}  + \\log\\prod_{i = 1}^{K}p_i^{x_i}\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation} = \\log{n \\choose{x_1, x_2...x_K}} + \\sum_{i = 1}^{k} \\log{p_i^{x_i}} \\end{equation}\n",
    "\n",
    "\\begin{equation} = \\log{n \\choose{x_1, x_2...x_K}} + \\sum_{i = 1}^{k}x_i\\log{p_i} \\end{equation}\n",
    "\n",
    "Since the above log likelihood function has a constrained $\\sum_{i = 1}^{K}p_i = 1$, we have to introduce constrained with a Lagrange multiplier $\\lambda$ into the equation.\n",
    "\n",
    "\\begin{equation}  \\mathcal{LL}(p, \\lambda) = \\mathcal{LL}(p) + \\lambda\\left( 1 - \\sum_{i=1}^Kp_i \\right)\\end{equation}\n",
    "\n",
    "\\begin{equation}  \\mathcal{LL}(p, \\lambda) = \\log{n \\choose{x_1, x_2...x_K}} + \\sum_{i = 1}^Kx_ilog{p_i} + \\lambda\\left( 1 - \\sum_{i=1}^Kp_i \\right)\\end{equation}\n",
    "\n",
    "$\\mathcal{LL}(p. \\lambda)$ is also called as Lagrangian.\n",
    "\n",
    "To find the $argmax_p \\mathcal{LL}(p, \\lambda)$ we have to differrentiate the Lagrangian function with respect to $p_i$ and $\\lambda$ and setting it $0$. \n",
    "\n",
    "\\begin{equation} \\frac{\\partial{}}{\\partial{p_i}} \\mathcal{LL}(p, \\lambda) = 0\\>\\> and \\>\\>\\frac{\\partial{}}{\\partial{\\lambda}} \\mathcal{LL}(p, \\lambda) = 0 \\end{equation}\n",
    "\n",
    "First differentiating it wrt  $\\lambda$ and setting it to $0$\n",
    "\n",
    "\\begin{equation} \\frac{\\partial{}}{\\partial{\\lambda}} \\mathcal{LL}(p, \\lambda)  = 0\\end{equation}\n",
    "\n",
    "\\begin{equation}  1 - \\sum_{i = 1}^Kp_i  = 0\\end{equation}\n",
    "\n",
    "\\begin{equation} \\sum_{i = 1}^{K}p_i = 1\\end{equation}\n",
    "\n",
    "Now differentiating it wrt  $p_i$ and setting it to $0$\n",
    "\n",
    "\\begin{equation} \\frac{\\partial{}}{\\partial{p_i}} \\mathcal{LL}(p, \\lambda)   = 0\\end{equation}\n",
    "\n",
    "\\begin{equation} \\frac{x_i}{p_i} + \\lambda(-1)  = 0\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation} x_i = \\lambda p_i  \\end{equation}\n",
    "\n",
    "\\begin{equation}p_i = \\frac{x_i}{\\lambda} ....(1)\\end{equation}\n",
    "\n",
    "Summning over all i\n",
    "\n",
    "\\begin{equation} \\sum_{i = 1}^{K}x_i = \\sum_{i = 1}^K\\lambda p_i \\end{equation}\n",
    "\n",
    "\\begin{equation} n = \\lambda ....(2)\\end{equation}\n",
    "\n",
    "Putting the value of $\\lambda$ from equation (2) to equation (1) we get, \n",
    "\n",
    "\\begin{equation} p_i = \\frac{x_i}{n} \\end{equation}\n",
    "\n",
    "Now, the probability distribution after MLE\n",
    "\n",
    "\\begin{equation} P\\left(\\frac{x_1}{n}, \\frac{x_2}{n}.....\\frac{x_K}{n}\\right) \\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
